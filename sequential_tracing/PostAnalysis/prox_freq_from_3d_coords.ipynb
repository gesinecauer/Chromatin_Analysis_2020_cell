{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='0'></a>\n",
    "# 0. Minimum required packages and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='0.1'></a>\n",
    "## 0.1 import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748464\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(r\"..\", r\".\")))\n",
    "\n",
    "import source as ia\n",
    "\n",
    "print(os.getpid()) # print this so u can terminate through cmd / task-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='0.2'></a>\n",
    "## 0.2 parameters for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plotting setting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', serif='Arial')\n",
    "_font_size = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plotting parameters\n",
    "from source.figure_tools import _dpi,_single_col_width,_double_col_width,_single_row_height,_ref_bar_length, _ticklabel_size,_ticklabel_width,_font_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.gc Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/zenodo_dl\n",
      "progress_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress\n",
      "counts_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/counts\n"
     ]
    }
   ],
   "source": [
    "# please change data_folder into the folder for downloaded dataset\n",
    "data_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"zenodo_dl\"))\n",
    "print(f\"data_folder: {data_folder}\")\n",
    "\n",
    "# [GC+] For saving partial progress\n",
    "progress_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"in_progress\"))\n",
    "print(f\"progress_folder: {progress_folder}\")\n",
    "os.makedirs(progress_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(progress_folder, '3d_coords'), exist_ok=True)\n",
    "os.makedirs(os.path.join(progress_folder, 'distances'), exist_ok=True)\n",
    "\n",
    "# [GC+] For saving final counts matrices\n",
    "counts_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"counts\"))\n",
    "print(f\"counts_folder: {counts_folder}\")\n",
    "os.makedirs(counts_folder, exist_ok=True)\n",
    "\n",
    "# # figure folder\n",
    "# # please specify location to save images\n",
    "# results_folder = os.path.abspath(os.path.join(r\"..\", r\"results\"))\n",
    "# parent_figure_folder = os.path.abspath(os.path.join(results_folder, \"figures\"))\n",
    "# os.makedirs(results_folder, exist_ok=True)\n",
    "# os.makedirs(parent_figure_folder, exist_ok=True)\n",
    "# print(f\"results_folder: {results_folder}\")\n",
    "# print(f\"parent_figure_folder: {parent_figure_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# please update as needed\n",
    "files_3dcoords = {\n",
    "    'chr21': {\n",
    "        'rep1': os.path.join(data_folder, 'chrom_trace',  'chr21.rep1.tsv'),\n",
    "        'rep2': os.path.join(data_folder, 'chrom_trace', 'chr21.rep2_cell_cycle.tsv')},\n",
    "    'chr2': {\n",
    "        'rep1': os.path.join(data_folder, 'chrom_trace',  'chr2.rep1.tsv'),\n",
    "        'rep2': os.path.join(data_folder, 'chrom_trace', 'chr2.rep2_p_arm.tsv')}}\n",
    "\n",
    "\n",
    "files_progress = {chrom: {} for chrom in files_3dcoords.keys()}\n",
    "\n",
    "for chrom in files_3dcoords.keys():\n",
    "    files_progress[chrom]['dir_coords3d'] = os.path.join(progress_folder, '3d_coords', chrom)\n",
    "    os.makedirs(files_progress[chrom]['dir_coords3d'], exist_ok=True)\n",
    "    \n",
    "    files_progress[chrom]['rep1'] = os.path.basename(files_3dcoords[chrom]['rep1']).replace('.tsv', '')\n",
    "    files_progress[chrom]['rep2'] = os.path.basename(files_3dcoords[chrom]['rep2']).replace('.tsv', '')\n",
    "    \n",
    "    for desc in ('distances.per_cell', 'distances.median', 'contacts.cutoff500'):\n",
    "        files_progress[chrom][desc] = os.path.join(progress_folder, 'distances', f'{chrom}.{desc}.npy')\n",
    "        \n",
    "\n",
    "files_progress['chr21']['proceed_with'] = 'chr21'\n",
    "files_progress['chr2']['proceed_with'] = files_progress['chr2']['rep1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please visit zenodo with DOI:10.5281/zenodo.3928890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1'></a>\n",
    "# Chromosome 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.gc'></a>\n",
    "## 1.gc Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr21\n",
      "chr21.rep1\n",
      "chr21.rep2_cell_cycle\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr21'\n",
    "rep1_filename = files_3dcoords[chrom]['rep1']\n",
    "rep2_filename = files_3dcoords[chrom]['rep2']\n",
    "rep1 = files_progress[chrom]['rep1']\n",
    "rep2 = files_progress[chrom]['rep2']\n",
    "print(chrom); print(rep1); print(rep2)\n",
    "\n",
    "dir_3d_coords =  files_progress[chrom]['dir_coords3d']\n",
    "distmap_list_file = files_progress[chrom]['distances.per_cell']\n",
    "median_distance_map_file = files_progress[chrom]['distances.median']\n",
    "median_distance_map_file = files_progress[chrom]['contacts.cutoff500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1 load chr21 replicate 1 \n",
    "\n",
    "(without cell cycle information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number', 'Gene names', 'Transcription', 'TSS ZXY(nm)']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3982c46d7dd04012bdfcd745313f45eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 genes exist in this dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4972ffbf8f6a4712aa02423f2e2b0f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/gs/vol1/home/gesine/local/anaconda3/envs/su2020/lib/python3.7/site-packages/ipykernel_launcher.py:101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep1_info_dict = {}\n",
    "with open(rep1_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep1_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep1_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep1 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep1_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep1_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep1_info_dict['Z(nm)'],rep1_info_dict['X(nm)'],\\\n",
    "                                         rep1_info_dict['Y(nm)'],rep1_info_dict['Genomic coordinate'],\\\n",
    "                                         rep1_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep1['chrom_ids'] = chr_ids\n",
    "data_rep1['region_names'] = region_names\n",
    "data_rep1['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep1['start_position_Mb'] = start_position_Mb\n",
    "data_rep1['end_position_Mb'] = end_position_Mb\n",
    "data_rep1['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep1_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep1_info_dict['Gene names'],\n",
    "                                                   rep1_info_dict['Transcription'],\n",
    "                                                   rep1_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep1_info_dict['Chromosome copy number'],\n",
    "                                                                  rep1_info_dict['Gene names'],\n",
    "                                                                  rep1_info_dict['Transcription'],\n",
    "                                                                  rep1_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep1['gene_names'] = gene_names\n",
    "    data_rep1['tss_zxys'] = tss_zxys_list\n",
    "    data_rep1['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep1_info_dict:\n",
    "    cell_cycle_types = np.unique(rep1_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep1_info_dict['Chromosome copy number'],rep1_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep1[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2 load chr21 replicate 2 \n",
    "\n",
    "(with cell cycle information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number', 'Gene names', 'Transcription', 'TSS ZXY(nm)', 'Cell cycle state']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f2af26b3aa422598fb70cf94f27336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 genes exist in this dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c1e18321c0460a86fc4dcb70bc00bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/gs/vol1/home/gesine/local/anaconda3/envs/su2020/lib/python3.7/site-packages/ipykernel_launcher.py:101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a011cb0ff2874886a7c7c548f0ec5244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep2_info_dict = {}\n",
    "with open(rep2_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep2_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep2_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep2 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep2_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep2_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep2_info_dict['Z(nm)'],rep2_info_dict['X(nm)'],\\\n",
    "                                         rep2_info_dict['Y(nm)'],rep2_info_dict['Genomic coordinate'],\\\n",
    "                                         rep2_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep2['chrom_ids'] = chr_ids\n",
    "data_rep2['region_names'] = region_names\n",
    "data_rep2['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep2['start_position_Mb'] = start_position_Mb\n",
    "data_rep2['end_position_Mb'] = end_position_Mb\n",
    "data_rep2['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep2_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep2_info_dict['Gene names'],\n",
    "                                                   rep2_info_dict['Transcription'],\n",
    "                                                   rep2_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep2_info_dict['Chromosome copy number'],\n",
    "                                                                  rep2_info_dict['Gene names'],\n",
    "                                                                  rep2_info_dict['Transcription'],\n",
    "                                                                  rep2_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep2['gene_names'] = gene_names\n",
    "    data_rep2['tss_zxys'] = tss_zxys_list\n",
    "    data_rep2['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep2_info_dict:\n",
    "    cell_cycle_types = np.unique(rep2_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep2_info_dict['Chromosome copy number'],rep2_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep2[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.3 combine two datasets\n",
    "\n",
    "in most of panels we combined chromosomes from two replicates for better statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chromosomes: 12149\n"
     ]
    }
   ],
   "source": [
    "data_combined = {}\n",
    "for _k in data_rep1:\n",
    "    if _k in data_rep2:\n",
    "        # concatenate if this is some chromosomal data\n",
    "        if len(data_rep1[_k]) == len(data_rep1[\"chrom_ids\"]):\n",
    "            data_combined[_k] = np.concatenate([data_rep1[_k],data_rep2[_k]])\n",
    "        # directly merge if this is some shared information\n",
    "        elif (np.array(data_rep1[_k]) == np.array(data_rep2[_k])).all():\n",
    "            data_combined[_k] = data_rep1[_k]\n",
    "            \n",
    "print('Number of chromosomes:', len(data_combined['chrom_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Save current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {chrom: data_combined, rep1: data_rep1, rep2: data_rep2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== chr21 ========\n",
      "chrom_ids            ndarray    (12149,)\n",
      "dna_zxys             ndarray    (12149, 651, 3)\n",
      "tss_zxys             ndarray    (12149, 84, 3)\n",
      "trans_pfs            ndarray    (12149, 84)\n",
      "\n",
      "======== chr21.rep1 ========\n",
      "chrom_ids            ndarray    (7591,)\n",
      "dna_zxys             ndarray    (7591, 651, 3)\n",
      "tss_zxys             ndarray    (7591, 84, 3)\n",
      "trans_pfs            ndarray    (7591, 84)\n",
      "\n",
      "======== chr21.rep2_cell_cycle ========\n",
      "chrom_ids            ndarray    (4558,)\n",
      "dna_zxys             ndarray    (4558, 651, 3)\n",
      "tss_zxys             ndarray    (4558, 84, 3)\n",
      "trans_pfs            ndarray    (4558, 84)\n",
      "G1_flags             ndarray    (4558,)\n",
      "G2/S_flags           ndarray    (4558,)\n",
      "\n",
      "======== BOTH REPS ========\n",
      "params               dict       0\n",
      "region_names         ndarray    (651,)\n",
      "mid_position_Mb      ndarray    (651,)\n",
      "start_position_Mb    ndarray    (651,)\n",
      "end_position_Mb      ndarray    (651,)\n",
      "gene_names           list       84\n",
      "\n",
      "\n",
      "Is 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\n",
      "chr21                     ✓\n",
      "chr21.rep1                ✓\n",
      "chr21.rep2_cell_cycle     ✓\n",
      "\n",
      "\n",
      "Confirm there aren't any weird rounding errors in 'start_position_Mb':\n",
      "chr21                     ✓\n",
      "chr21.rep1                ✓\n",
      "chr21.rep2_cell_cycle     ✓\n"
     ]
    }
   ],
   "source": [
    "# [GC+] Assess data structure\n",
    "in_common = False\n",
    "for name, data in all_data.items():\n",
    "    print(f'\\n======== {name} ========')\n",
    "    for key, val in data.items():\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            in_common = True\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "\n",
    "if in_common:\n",
    "    print(f'\\n======== BOTH REPS ========')\n",
    "    for key, val in data_rep1.items():\n",
    "        if not (key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key])):\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "            \n",
    "\n",
    "print(\"\\n\\nIs 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\")\n",
    "for name, data in all_data.items():\n",
    "    if 'chrom_ids' in data:\n",
    "        tmp = np.array_equal(data_rep1['chrom_ids'], np.arange(1, len(data_rep1['chrom_ids']) + 1))\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[tmp])\n",
    "\n",
    "print(\"\\n\\nConfirm there aren't any weird rounding errors in 'start_position_Mb':\")\n",
    "for name, data in all_data.items():\n",
    "    if 'start_position_Mb' in data:\n",
    "        tmp = (data['start_position_Mb'] - np.round(data['start_position_Mb'], 4)) != 0\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[~np.any(tmp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10400001 10500001 10600001 13250001 14000001 14050001 14100001 14150001\n",
      " 14200001 14250001]\n",
      "[10400000 10500000 10600000 13250000 14000000 14050000 14100000 14150000\n",
      " 14200000 14250000]\n",
      "[10.4  10.5  10.6  13.25 14.   14.05 14.1  14.15 14.2  14.25]\n",
      "[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05]\n"
     ]
    }
   ],
   "source": [
    "print((region_starts)[:10])\n",
    "print((region_starts - 1)[:10])\n",
    "print(((region_starts - 1)[:10].astype(int) / 10 ** 6))\n",
    "print(np.round((region_starts - 1)[:10].astype(int) / 10 ** 6, 4) % 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.chrom_ids.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.G1_flags.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.G2-S_flags.npy\n"
     ]
    }
   ],
   "source": [
    "# [GC+] SAVE in-progress files - numpy\n",
    "\n",
    "for name, data in all_data.items():\n",
    "    for key, val in data.items():\n",
    "        if len(val) == 0 or key in ('start_position_Mb', 'end_position_Mb') or np.array_equal(np.asarray(val).ravel(), np.arange(np.asarray(val).size) + 1):\n",
    "            continue\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            tmp = chrom\n",
    "        else:\n",
    "            tmp = name\n",
    "        if isinstance(val, dict):\n",
    "            pass\n",
    "        else:\n",
    "            key = key.replace('/', '-')\n",
    "            print(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"))\n",
    "            np.save(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GC+] SAVE in-progress files - pickled\n",
    "with open(os.path.join(dir_3d_coords, rep1) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, rep2) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, chrom) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_combined, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "\n",
    "# with open(rep1_coords_file, 'rb') as handle:\n",
    "#     data_rep1 = pickle.load(handle)\n",
    "# with open(rep2_coords_file, 'rb') as handle:\n",
    "#     data_rep2 = pickle.load(handle)\n",
    "\n",
    "# with open(combo_coords_file, 'rb') as handle:\n",
    "#     data_combined = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1 generate imaging-based median distance and proximity frequency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# zxys_combined_list = np.array(data_combined['dna_zxys'])\n",
    "# distmap_combined_list = np.array([squareform(pdist(_zxy)) for _zxy in tqdm(zxys_combined_list)])\n",
    "\n",
    "# # generate median distance map\n",
    "# median_distance_map_combined = np.nanmedian(distmap_combined_list, axis = 0)\n",
    "# # generate contact map\n",
    "# contact_th = 500\n",
    "# contact_map_combined = np.sum(distmap_combined_list<contact_th, axis=0) / np.sum(np.isnan(distmap_combined_list)==False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f8419b994c4ab6b9415b50ace95fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd9572a92514ec09510e1ed5ca8fd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [GC+] edited so I can SAVE in-progress files\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "zxys_combined_list = np.array(data_combined['dna_zxys'])\n",
    "\n",
    "distmap_combined_list = np.concatenate([pdist(_zxy).reshape(1, -1) for _zxy in tqdm(zxys_combined_list)])\n",
    "np.save(distmap_list_file, distmap_combined_list) # [GC+] SAVE in-progress files\n",
    "\n",
    "distmap_combined_list = np.array([squareform(distvec) for distvec in tqdm(distmap_combined_list)])\n",
    "\n",
    "# generate median distance map\n",
    "median_distance_map_combined = np.nanmedian(distmap_combined_list, axis = 0)\n",
    "np.save(median_distance_map_file, median_distance_map_combined) # [GC+] SAVE in-progress files\n",
    "\n",
    "# generate contact map\n",
    "contact_th = 500\n",
    "contact_map_combined = np.sum(distmap_combined_list<contact_th, axis=0) / np.sum(np.isnan(distmap_combined_list)==False, axis=0)\n",
    "np.save(median_distance_map_file, contact_map_combined) # [GC+] SAVE in-progress files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "# distmap_combined_list = np.load(distmap_list_file)\n",
    "# median_distance_map_combined = np.load(median_distance_map_file)\n",
    "# contact_map_combined = np.load(median_distance_map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1'></a>\n",
    "# Chromosome 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.gc'></a>\n",
    "## 1.gc Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2\n",
      "chr2.rep1\n",
      "chr2.rep2_p_arm\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr2'\n",
    "rep1_filename = files_3dcoords[chrom]['rep1']\n",
    "rep2_filename = files_3dcoords[chrom]['rep2']\n",
    "rep1 = files_progress[chrom]['rep1']\n",
    "rep2 = files_progress[chrom]['rep2']\n",
    "print(chrom); print(rep1); print(rep2)\n",
    "\n",
    "dir_3d_coords =  files_progress[chrom]['dir_coords3d']\n",
    "distmap_list_file = files_progress[chrom]['distances.per_cell']\n",
    "median_distance_map_file = files_progress[chrom]['distances.median']\n",
    "median_distance_map_file = files_progress[chrom]['contacts.cutoff500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1 load chr2 replicate 1 \n",
    "\n",
    "(entire chr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71554eb51e0d482e87db8e3b2d1f70df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep1_info_dict = {}\n",
    "with open(rep1_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep1_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep1_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep1 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep1_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep1_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep1_info_dict['Z(nm)'],rep1_info_dict['X(nm)'],\\\n",
    "                                         rep1_info_dict['Y(nm)'],rep1_info_dict['Genomic coordinate'],\\\n",
    "                                         rep1_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep1['chrom_ids'] = chr_ids\n",
    "data_rep1['region_names'] = region_names\n",
    "data_rep1['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep1['start_position_Mb'] = start_position_Mb\n",
    "data_rep1['end_position_Mb'] = end_position_Mb\n",
    "data_rep1['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep1_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep1_info_dict['Gene names'],\n",
    "                                                   rep1_info_dict['Transcription'],\n",
    "                                                   rep1_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep1_info_dict['Chromosome copy number'],\n",
    "                                                                  rep1_info_dict['Gene names'],\n",
    "                                                                  rep1_info_dict['Transcription'],\n",
    "                                                                  rep1_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep1['gene_names'] = gene_names\n",
    "    data_rep1['tss_zxys'] = tss_zxys_list\n",
    "    data_rep1['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep1_info_dict:\n",
    "    cell_cycle_types = np.unique(rep1_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep1_info_dict['Chromosome copy number'],rep1_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep1[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2 load chr2 replicate 2 \n",
    "\n",
    "(p-arm only, first 357 regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab24d76d64304a42b56008b43ff92529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep2_info_dict = {}\n",
    "with open(rep2_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep2_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep2_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep2 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep2_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep2_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep2_info_dict['Z(nm)'],rep2_info_dict['X(nm)'],\\\n",
    "                                         rep2_info_dict['Y(nm)'],rep2_info_dict['Genomic coordinate'],\\\n",
    "                                         rep2_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep2['chrom_ids'] = chr_ids\n",
    "data_rep2['region_names'] = region_names\n",
    "data_rep2['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep2['start_position_Mb'] = start_position_Mb\n",
    "data_rep2['end_position_Mb'] = end_position_Mb\n",
    "data_rep2['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep2_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep2_info_dict['Gene names'],\n",
    "                                                   rep2_info_dict['Transcription'],\n",
    "                                                   rep2_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep2_info_dict['Chromosome copy number'],\n",
    "                                                                  rep2_info_dict['Gene names'],\n",
    "                                                                  rep2_info_dict['Transcription'],\n",
    "                                                                  rep2_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep2['gene_names'] = gene_names\n",
    "    data_rep2['tss_zxys'] = tss_zxys_list\n",
    "    data_rep2['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep2_info_dict:\n",
    "    cell_cycle_types = np.unique(rep2_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep2_info_dict['Chromosome copy number'],rep2_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep2[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Save current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {rep1: data_rep1, rep2: data_rep2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== chr2.rep1 ========\n",
      "chrom_ids            ndarray    (3029,)\n",
      "region_names         ndarray    (935,)\n",
      "mid_position_Mb      ndarray    (935,)\n",
      "start_position_Mb    ndarray    (935,)\n",
      "end_position_Mb      ndarray    (935,)\n",
      "dna_zxys             ndarray    (3029, 935, 3)\n",
      "\n",
      "======== chr2.rep2_p_arm ========\n",
      "chrom_ids            ndarray    (4848,)\n",
      "region_names         ndarray    (357,)\n",
      "mid_position_Mb      ndarray    (357,)\n",
      "start_position_Mb    ndarray    (357,)\n",
      "end_position_Mb      ndarray    (357,)\n",
      "dna_zxys             ndarray    (4848, 357, 3)\n",
      "\n",
      "======== BOTH REPS ========\n",
      "params               dict       0\n",
      "\n",
      "\n",
      "Is 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\n",
      "chr2.rep1           : True\n",
      "chr2.rep2_p_arm     : True\n",
      "\n",
      "\n",
      "Are there weird rounding errors in 'start_position_Mb'?\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# [GC+] Assess data structure\n",
    "in_common = False\n",
    "for name, data in all_data.items():\n",
    "    print(f'\\n======== {name} ========')\n",
    "    for key, val in data.items():\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            in_common = True\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "\n",
    "if in_common:\n",
    "    print(f'\\n======== BOTH REPS ========')\n",
    "    for key, val in data_rep1.items():\n",
    "        if not (key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key])):\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "            \n",
    "\n",
    "print(\"\\n\\nIs 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\")\n",
    "for name, data in all_data.items():\n",
    "    if 'chrom_ids' in data:\n",
    "        tmp = np.array_equal(data_rep1['chrom_ids'], np.arange(1, len(data_rep1['chrom_ids']) + 1))\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[tmp])\n",
    "\n",
    "print(\"\\n\\nConfirm there aren't any weird rounding errors in 'start_position_Mb':\")\n",
    "for name, data in all_data.items():\n",
    "    if 'start_position_Mb' in data:\n",
    "        tmp = (data['start_position_Mb'] - np.round(data['start_position_Mb'], 4)) != 0\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[~np.any(tmp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.dna_zxys.npy\n"
     ]
    }
   ],
   "source": [
    "# [GC+] SAVE in-progress files - numpy\n",
    "\n",
    "for name, data in all_data.items():\n",
    "    for key, val in data.items():\n",
    "        if len(val) == 0 or key in ('start_position_Mb', 'end_position_Mb') or np.array_equal(np.asarray(val).ravel(), np.arange(np.asarray(val).size) + 1):\n",
    "            continue\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            tmp = chrom\n",
    "        else:\n",
    "            tmp = name\n",
    "        if isinstance(val, dict):\n",
    "            pass\n",
    "        else:\n",
    "            key = key.replace('/', '-')\n",
    "            print(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"))\n",
    "            np.save(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GC+] SAVE in-progress files - pickled\n",
    "with open(os.path.join(dir_3d_coords, rep1) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, rep2) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "\n",
    "# with open(rep1_coords_file, 'rb') as handle:\n",
    "#     data_rep1 = pickle.load(handle)\n",
    "# with open(rep2_coords_file, 'rb') as handle:\n",
    "#     data_rep2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.0 corresponding regions for p and q arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # P and q arm crop\n",
    "# p_crop = slice(0, 357)\n",
    "# q_crop = slice(357, len(data_rep1['dna_zxys'][0]))\n",
    "# print(p_crop, q_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1 generate imaging-based median distance and proximity frequency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import squareform, pdist\n",
    "# zxys_rep1_list = np.array(data_rep1['dna_zxys'])\n",
    "# distmap_rep1_list = np.array([squareform(pdist(_zxy)) for _zxy in zxys_rep1_list])\n",
    "# # calculate contact freq map\n",
    "# contact_th = 500\n",
    "# contact_rep1_map = np.sum(distmap_rep1_list<contact_th, axis=0) / np.sum(np.isnan(distmap_rep1_list)==False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69317f56bf404b67816c987021d07654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968a5249b4849e5acdf5f7e19ae9cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [GC+] edited so I can SAVE in-progress files\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "zxys_rep1_list = np.array(data_rep1['dna_zxys'])\n",
    "\n",
    "distmap_rep1_list = np.concatenate([pdist(_zxy).reshape(1, -1) for _zxy in tqdm(zxys_rep1_list)])\n",
    "np.save(distmap_list_file, distmap_rep1_list) # [GC+] SAVE in-progress files\n",
    "\n",
    "distmap_rep1_list = np.array([squareform(distvec) for distvec in tqdm(distmap_rep1_list)])\n",
    "\n",
    "# generate median distance map\n",
    "median_distance_map_rep1 = np.nanmedian(distmap_rep1_list, axis = 0)\n",
    "np.save(median_distance_map_file, median_distance_map_rep1) # [GC+] SAVE in-progress files\n",
    "\n",
    "# generate contact map\n",
    "contact_th = 500\n",
    "contact_map_rep1 = np.sum(distmap_rep1_list<contact_th, axis=0) / np.sum(np.isnan(distmap_rep1_list)==False, axis=0)\n",
    "np.save(median_distance_map_file, contact_map_rep1) # [GC+] SAVE in-progress files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "# distmap_combined_list = np.load(distmap_list_file)\n",
    "# median_distance_map_combined = np.load(median_distance_map_file)\n",
    "# contact_map_combined = np.load(median_distance_map_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create proper hiclib-style counts matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dir_coords3d', 'rep1', 'rep2', 'distances.per_cell', 'distances.median', 'contacts.cutoff500', 'proceed_with'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Quick check, make sure all regions are 0.05 Mb\n",
    "for chrom in files_progress.keys():\n",
    "    region_names = np.load(os.path.join(files_progress[chrom]['dir_coords3d'], files_progress[chrom]['proceed_with'] + '.region_names.npy'))\n",
    "    regions = np.concatenate([np.array(re.split(r'[:-]', x)[1:], dtype=int).reshape(1, -1) for x in region_names])\n",
    "    region_sizes = np.unique(regions[:,1] - regions[:,0]) / 1e6\n",
    "    if region_sizes.size != 1 or region_sizes[0] != 0.05:\n",
    "        raise ValueError('Region sizes not all equal to 0.05 Mb')\n",
    "        \n",
    "files_progress[chrom].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', message='', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', message='', category=FutureWarning)\n",
    "    from iced.io import write_counts, write_lengths\n",
    "\n",
    "def get_complete_counts_matrix(files_progress, chrom, good_region_th=0.25, res=0.05, verbose=True, save=False):\n",
    "    # Which of the bins are NaN < good_region_th * 100% of the time?\n",
    "    dna_zxys = np.load(os.path.join(files_progress[chrom]['dir_coords3d'], files_progress[chrom]['proceed_with'] + '.dna_zxys.npy'))\n",
    "    failure_rates = 1 - np.mean(np.isnan(dna_zxys).sum(2)==0, axis=0)\n",
    "    good_regions = np.where(failure_rates < good_region_th)[0]\n",
    "\n",
    "    # Get mids that pass cutoff\n",
    "    mids_orig = np.load(os.path.join(files_progress[chrom]['dir_coords3d'], files_progress[chrom]['proceed_with'] + '.mid_position_Mb.npy'))\n",
    "    if not np.array_equal(mids_orig, np.sort(mids_orig)):\n",
    "        raise ValueError('Midpoints not in ascending order')\n",
    "    mids_good = mids_orig[good_regions]\n",
    "\n",
    "    # Get counts that pass cutoff\n",
    "    counts_orig = np.triu(np.load(os.path.join(files_progress[chrom]['contacts.cutoff500'])), 1)\n",
    "    counts_good = counts_orig[good_regions][:,good_regions]\n",
    "\n",
    "    # Create complete counts matrix\n",
    "    mids_all = np.round(np.arange(mids_good.min(), mids_good.max() + res, res), 6)\n",
    "    idx = np.where(np.isin(mids_all, mids_good))[0]\n",
    "    counts = np.zeros((mids_all.size, mids_all.size), dtype=int)\n",
    "    counts[idx][:,idx] = counts_good\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Chrom={chrom.ljust(9)} Cutoff={str(good_region_th).ljust(7)} Coverage={idx.size / mids_all.size * 100:.3g}%\")\n",
    "        \n",
    "    if save:\n",
    "        outdir = os.path.join(counts_folder, f\"{chrom}.cutoff{good_region_th}\")\n",
    "        write_counts(os.path.join(outdir, \"counts.matrix\"), counts)\n",
    "        write_lengths(os.path.join(outdir, \"counts.bed\"), np.array([counts.shape[0]]))\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrom=chr21     Cutoff=1       Coverage=89.7%\n",
      "Chrom=chr21     Cutoff=0.25    Coverage=89%\n",
      "Chrom=chr21     Cutoff=0.2     Coverage=88.6%\n",
      "Chrom=chr21     Cutoff=0.1     Coverage=82.9%\n",
      "Chrom=chr21     Cutoff=0.05    Coverage=62.8%\n",
      "Chrom=chr21     Cutoff=0.025   Coverage=16%\n",
      "Chrom=chr2      Cutoff=1       Coverage=19.3%\n",
      "Chrom=chr2      Cutoff=0.25    Coverage=19.3%\n",
      "Chrom=chr2      Cutoff=0.2     Coverage=19.3%\n",
      "Chrom=chr2      Cutoff=0.1     Coverage=19.3%\n",
      "Chrom=chr2      Cutoff=0.05    Coverage=19.3%\n",
      "Chrom=chr2      Cutoff=0.025   Coverage=19.3%\n"
     ]
    }
   ],
   "source": [
    "for chrom in files_progress.keys():\n",
    "    for cutoff in (1, 0.25, 0.2, 0.1, 0.05, 0.025):\n",
    "        counts = get_complete_counts_matrix(files_progress, chrom=chrom, good_region_th=cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrom=chr21     Cutoff=0.1     Coverage=82.9%\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr21'\n",
    "good_region_th = 0.1\n",
    "\n",
    "counts = get_complete_counts_matrix(files_progress, chrom=chrom, good_region_th=good_region_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom = 'chr2'\n",
    "res = 0.05\n",
    "\n",
    "mids_orig = np.load(os.path.join(files_progress[chrom]['dir_coords3d'], files_progress[chrom]['proceed_with'] + '.mid_position_Mb.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (np.round(mids_orig * 1e3)).astype(int)\n",
    "to_next_bin = ((foo[1:] - foo[:-1]) / int(res * 1e3)).astype(int)\n",
    "# gaps_mask = np.where(to_next_bin != 250)[0]\n",
    "# print(np.stack([gaps_mask, to_next_bin[gaps_mask]], axis=1))\n",
    "# # print(to_next_bin[gaps_mask])\n",
    "\n",
    "print((foo[1:] - foo[:-1]).min() / int(res * 1e3))\n",
    "(foo[1:] - foo[:-1]).min(), int(res * 1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5., 10.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5., 10.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5., 10.,  5., 10.,  5.,  5.,  5.,\n",
       "        5., 15.,  5., 35., 60.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5., 15.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5., 25.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5., 10.,  5.,\n",
       "       10.,  5.,  5., 10.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5., 10.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5., 10.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_next_bin = ((mids_orig[1:] - mids_orig[:-1]) / res)\n",
    "# gaps_mask = np.where(to_next_bin != 250)[0]\n",
    "# print(np.stack([gaps_mask, to_next_bin[gaps_mask]], axis=1))\n",
    "# # print(to_next_bin[gaps_mask])\n",
    "\n",
    "# print((foo[1:] - foo[:-1]).min() / int(res *e3))\n",
    "# (foo[1:] - foo[:-1]).min(), int(res * 1e3)\n",
    "\n",
    "to_next_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.025],\n",
       "       [0.275],\n",
       "       [0.525],\n",
       "       [0.775],\n",
       "       [1.025],\n",
       "       [1.275],\n",
       "       [1.525],\n",
       "       [1.775],\n",
       "       [2.025],\n",
       "       [2.275]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mids_good = mids_orig\n",
    "mids_all = np.round(np.arange(mids_good.min(), mids_good.max() + res, res), 6)\n",
    "gaps_mask, mids_all.shape, mids_good.shape\n",
    "\n",
    "\n",
    "# # P and q arm crop\n",
    "# p_crop = slice(0, 357)\n",
    "# q_crop = slice(357, len(data_rep1['dna_zxys'][0]))\n",
    "# print(p_crop, q_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(0, 10, None)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
