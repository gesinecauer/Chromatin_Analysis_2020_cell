{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='0'></a>\n",
    "# 0. Minimum required packages and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='0.1'></a>\n",
    "## 0.1 import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727359\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# import source as ia # As of 3/14/25... ModuleNotFoundError: No module named 'source' ???\n",
    "\n",
    "print(os.getpid()) # print this so u can terminate through cmd / task-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.gc Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/zenodo_dl\n",
      "progress_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress\n",
      "counts_folder: /net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/counts\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(r\"..\", r\".\")))\n",
    "\n",
    "# please change data_folder into the folder for downloaded dataset\n",
    "data_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"zenodo_dl\"))\n",
    "print(f\"data_folder: {data_folder}\")\n",
    "\n",
    "# [GC+] For saving partial progress\n",
    "progress_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"in_progress\"))\n",
    "print(f\"progress_folder: {progress_folder}\")\n",
    "os.makedirs(progress_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(progress_folder, '3d_coords'), exist_ok=True)\n",
    "os.makedirs(os.path.join(progress_folder, 'distances'), exist_ok=True)\n",
    "\n",
    "# [GC+] For saving final counts matrices\n",
    "counts_folder = os.path.abspath(os.path.join(r\"..\", r\"data\", r\"counts\"))\n",
    "print(f\"counts_folder: {counts_folder}\")\n",
    "os.makedirs(counts_folder, exist_ok=True)\n",
    "\n",
    "# # figure folder\n",
    "# # please specify location to save images\n",
    "# results_folder = os.path.abspath(os.path.join(r\"..\", r\"results\"))\n",
    "# parent_figure_folder = os.path.abspath(os.path.join(results_folder, \"figures\"))\n",
    "# os.makedirs(results_folder, exist_ok=True)\n",
    "# os.makedirs(parent_figure_folder, exist_ok=True)\n",
    "# print(f\"results_folder: {results_folder}\")\n",
    "# print(f\"parent_figure_folder: {parent_figure_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# please update as needed\n",
    "files_3dcoords = {\n",
    "    'chr21': {\n",
    "        'rep1': os.path.join(data_folder, 'chrom_trace',  'chr21.rep1.tsv'),\n",
    "        'rep2': os.path.join(data_folder, 'chrom_trace', 'chr21.rep2_cell_cycle.tsv')},\n",
    "    'chr2': {\n",
    "        'rep1': os.path.join(data_folder, 'chrom_trace',  'chr2.rep1.tsv'),\n",
    "        'rep2': os.path.join(data_folder, 'chrom_trace', 'chr2.rep2_p_arm.tsv')}}\n",
    "\n",
    "\n",
    "files_dict = {chrom: {} for chrom in files_3dcoords.keys()}\n",
    "\n",
    "for chrom in files_3dcoords.keys():\n",
    "    files_dict[chrom]['dir_coords3d'] = os.path.join(progress_folder, '3d_coords', chrom)\n",
    "    os.makedirs(files_dict[chrom]['dir_coords3d'], exist_ok=True)\n",
    "    \n",
    "    files_dict[chrom]['rep1'] = os.path.basename(files_3dcoords[chrom]['rep1']).replace('.tsv', '')\n",
    "    files_dict[chrom]['rep2'] = os.path.basename(files_3dcoords[chrom]['rep2']).replace('.tsv', '')\n",
    "    \n",
    "    for desc in ('distances.vector_per_cell', 'distances.median', 'distances.mean', 'counts.mean.cutoff500', 'counts.vector_per_cell.cutoff500'):\n",
    "        files_dict[chrom][desc] = os.path.join(progress_folder, 'distances', f'{chrom}.{desc}.npy')\n",
    "        \n",
    "\n",
    "files_dict['chr21']['proceed_with'] = 'chr21'\n",
    "files_dict['chr2']['proceed_with'] = files_dict['chr2']['rep1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please visit zenodo with DOI:10.5281/zenodo.3928890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0.gc Function for working with sc distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sc_distances(filenames_chrom, sc_dist_vec=None, dis_mean=True, dis_median=True, contact_th=500, verbose=True):\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    # sc_dist_vec_file = filenames_chrom['distances.vector_per_cell']\n",
    "    # median_dist_matrix_file = filenames_chrom['distances.median']\n",
    "    # mean_dist_matrix_file = filenames_chrom['distances.mean']\n",
    "    # mean_counts_matrix_file = filenames_chrom[f'counts.mean.cutoff{contact_th}']\n",
    "    # sc_counts_vec_file = filenames_chrom[f'counts.vector_per_cell.cutoff{contact_th}']\n",
    "\n",
    "    if sc_dist_vec is None:\n",
    "        if verbose: print('Loading sc distances...')\n",
    "        sc_dist_vec = np.load(filenames_chrom['distances.vector_per_cell'])\n",
    "\n",
    "    # print(\"Converting distance vectors to 'squareform' matrices...\")\n",
    "    # distmap_per_cell = np.stack([squareform(distvec) for distvec in sc_dist_vec])\n",
    "\n",
    "    if dis_median:\n",
    "        if verbose: print('Generate median distances across cells...')\n",
    "        median_dist_matrix = squareform(np.nanmedian(sc_dist_vec, axis=0))\n",
    "        np.save(filenames_chrom['distances.median'], median_dist_matrix) # [GC+] SAVE in-progress files\n",
    "\n",
    "    if dis_mean: # [GC+] generate MEAN distance map\n",
    "        if verbose: print('Generate mean distances across cells...')\n",
    "        mean_dist_matrix = squareform(np.nanmean(sc_dist_vec, axis=0))\n",
    "        np.save(filenames_chrom['distances.mean'], mean_dist_matrix) # [GC+] SAVE in-progress files\n",
    "\n",
    "    if contact_th is not None:\n",
    "        if verbose: print('Generate counts...')\n",
    "        sc_counts_vec = (sc_dist_vec<contact_th).astype(int)\n",
    "        if verbose: print('                 ...saving contacts<thresh per cell') # [GC+] generate contacts<thresh PER-CELL\n",
    "        np.save(filenames_chrom[f'counts.vector_per_cell.cutoff{contact_th}'], sc_counts_vec)\n",
    "        # mean_counts_matrix = np.sum(sc_counts_vec, axis=0) / np.sum(np.isnan(sc_dist_vec)==False, axis=0)\n",
    "        mean_counts_matrix = squareform(np.nanmean(sc_counts_vec, axis=0))\n",
    "        if verbose: print('                 ...saving mean contacts across cells')\n",
    "        np.save(filenames_chrom[f'counts.mean.cutoff{contact_th}'], mean_counts_matrix) # [GC+] SAVE in-progress files\n",
    "\n",
    "    if verbose: print('Done!')\n",
    "    \n",
    "    # print(\"Converting contact matrices to vectors...\") # Can be done with squareform or with triu_indices\n",
    "    # # sc_counts_vec = np.stack([squareform(c_matrix) for c_matrix in sc_counts_matrices]) # equivalent to the below\n",
    "    # nbins = sc_counts_matrices.shape[-1]\n",
    "    # triu_idx = np.triu_indices(nbins, 1)\n",
    "    # sc_counts_vec = sc_counts_matrices[:, triu_idx[0], triu_idx[1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='0.2'></a>\n",
    "## 0.2 parameters for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plotting setting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', serif='Arial')\n",
    "_font_size = 7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required plotting parameters\n",
    "from source.figure_tools import _dpi,_single_col_width,_double_col_width,_single_row_height,_ref_bar_length, _ticklabel_size,_ticklabel_width,_font_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 0.gc Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming that squareform is the same as using np.triu etc\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "size = 40\n",
    "arr = np.arange(size ** 2).reshape(size, size)\n",
    "arr = arr + arr.T\n",
    "np.fill_diagonal(arr, 0)\n",
    "\n",
    "arr_vec1 = squareform(arr)\n",
    "triu_idx = np.triu_indices(size, 1)\n",
    "arr_vec2 = arr[triu_idx]\n",
    "print(np.array_equal(arr_vec1, arr_vec2))\n",
    "\n",
    "arr_vec_arr1 = squareform(arr_vec1)\n",
    "arr_vec_arr2 = np.zeros([size, size])\n",
    "arr_vec_arr2[np.triu_indices(size, 1)] = arr_vec2\n",
    "arr_vec_arr2 += arr_vec_arr2.T\n",
    "print(np.array_equal(arr_vec_arr1, arr_vec_arr2))\n",
    "\n",
    "arrB = arr + 1\n",
    "arrC = arr + 1\n",
    "np.fill_diagonal(arrB, 0)\n",
    "np.fill_diagonal(arrC, 0)\n",
    "arrs = np.stack([arr, arrB, arrC])\n",
    "\n",
    "arrs_vec1 = np.stack([squareform(x) for x in arrs])\n",
    "arrs_vec2 = arrs[:, triu_idx[0], triu_idx[1]]\n",
    "print(np.array_equal(arrs_vec1, arrs_vec2))\n",
    "\n",
    "arrs.shape, arrs_vec2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1'></a>\n",
    "# Chromosome 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.gc'></a>\n",
    "## 1.gc Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr21\n",
      "chr21.rep1\n",
      "chr21.rep2_cell_cycle\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr21'\n",
    "rep1_filename = files_3dcoords[chrom]['rep1']\n",
    "rep2_filename = files_3dcoords[chrom]['rep2']\n",
    "rep1 = files_dict[chrom]['rep1']\n",
    "rep2 = files_dict[chrom]['rep2']\n",
    "print(chrom); print(rep1); print(rep2)\n",
    "\n",
    "dir_3d_coords =  files_dict[chrom]['dir_coords3d']\n",
    "distmap_list_file = files_dict[chrom]['distances.vector_per_cell']\n",
    "median_dist_matrix_file = files_dict[chrom]['distances.median']\n",
    "mean_dist_matrix_file = files_dict[chrom]['distances.mean']\n",
    "mean_counts_matrix_c500_file = files_dict[chrom]['counts.mean.cutoff500']\n",
    "sc_counts_vec_c500_file = files_dict[chrom]['counts.vector_per_cell.cutoff500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1 load chr21 replicate 1 \n",
    "\n",
    "(without cell cycle information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number', 'Gene names', 'Transcription', 'TSS ZXY(nm)']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3982c46d7dd04012bdfcd745313f45eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 genes exist in this dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4972ffbf8f6a4712aa02423f2e2b0f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/gs/vol1/home/gesine/local/anaconda3/envs/su2020/lib/python3.7/site-packages/ipykernel_launcher.py:101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep1_info_dict = {}\n",
    "with open(rep1_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep1_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep1_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep1 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep1_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep1_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep1_info_dict['Z(nm)'],rep1_info_dict['X(nm)'],\\\n",
    "                                         rep1_info_dict['Y(nm)'],rep1_info_dict['Genomic coordinate'],\\\n",
    "                                         rep1_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep1['chrom_ids'] = chr_ids\n",
    "data_rep1['region_names'] = region_names\n",
    "data_rep1['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep1['start_position_Mb'] = start_position_Mb\n",
    "data_rep1['end_position_Mb'] = end_position_Mb\n",
    "data_rep1['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep1_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep1_info_dict['Gene names'],\n",
    "                                                   rep1_info_dict['Transcription'],\n",
    "                                                   rep1_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep1_info_dict['Chromosome copy number'],\n",
    "                                                                  rep1_info_dict['Gene names'],\n",
    "                                                                  rep1_info_dict['Transcription'],\n",
    "                                                                  rep1_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep1['gene_names'] = gene_names\n",
    "    data_rep1['tss_zxys'] = tss_zxys_list\n",
    "    data_rep1['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep1_info_dict:\n",
    "    cell_cycle_types = np.unique(rep1_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep1_info_dict['Chromosome copy number'],rep1_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep1[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2 load chr21 replicate 2 \n",
    "\n",
    "(with cell cycle information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number', 'Gene names', 'Transcription', 'TSS ZXY(nm)', 'Cell cycle state']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f2af26b3aa422598fb70cf94f27336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 genes exist in this dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c1e18321c0460a86fc4dcb70bc00bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/gs/vol1/home/gesine/local/anaconda3/envs/su2020/lib/python3.7/site-packages/ipykernel_launcher.py:101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a011cb0ff2874886a7c7c548f0ec5244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep2_info_dict = {}\n",
    "with open(rep2_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep2_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep2_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep2 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep2_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep2_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep2_info_dict['Z(nm)'],rep2_info_dict['X(nm)'],\\\n",
    "                                         rep2_info_dict['Y(nm)'],rep2_info_dict['Genomic coordinate'],\\\n",
    "                                         rep2_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep2['chrom_ids'] = chr_ids\n",
    "data_rep2['region_names'] = region_names\n",
    "data_rep2['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep2['start_position_Mb'] = start_position_Mb\n",
    "data_rep2['end_position_Mb'] = end_position_Mb\n",
    "data_rep2['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep2_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep2_info_dict['Gene names'],\n",
    "                                                   rep2_info_dict['Transcription'],\n",
    "                                                   rep2_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep2_info_dict['Chromosome copy number'],\n",
    "                                                                  rep2_info_dict['Gene names'],\n",
    "                                                                  rep2_info_dict['Transcription'],\n",
    "                                                                  rep2_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep2['gene_names'] = gene_names\n",
    "    data_rep2['tss_zxys'] = tss_zxys_list\n",
    "    data_rep2['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep2_info_dict:\n",
    "    cell_cycle_types = np.unique(rep2_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep2_info_dict['Chromosome copy number'],rep2_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep2[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.3 combine two datasets\n",
    "\n",
    "in most of panels we combined chromosomes from two replicates for better statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chromosomes: 12149\n"
     ]
    }
   ],
   "source": [
    "data_combined = {}\n",
    "for _k in data_rep1:\n",
    "    if _k in data_rep2:\n",
    "        # concatenate if this is some chromosomal data\n",
    "        if len(data_rep1[_k]) == len(data_rep1[\"chrom_ids\"]):\n",
    "            data_combined[_k] = np.concatenate([data_rep1[_k],data_rep2[_k]])\n",
    "        # directly merge if this is some shared information\n",
    "        elif (np.array(data_rep1[_k]) == np.array(data_rep2[_k])).all():\n",
    "            data_combined[_k] = data_rep1[_k]\n",
    "            \n",
    "print('Number of chromosomes:', len(data_combined['chrom_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Save current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {chrom: data_combined, rep1: data_rep1, rep2: data_rep2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== chr21 ========\n",
      "chrom_ids            ndarray    (12149,)\n",
      "dna_zxys             ndarray    (12149, 651, 3)\n",
      "tss_zxys             ndarray    (12149, 84, 3)\n",
      "trans_pfs            ndarray    (12149, 84)\n",
      "\n",
      "======== chr21.rep1 ========\n",
      "chrom_ids            ndarray    (7591,)\n",
      "dna_zxys             ndarray    (7591, 651, 3)\n",
      "tss_zxys             ndarray    (7591, 84, 3)\n",
      "trans_pfs            ndarray    (7591, 84)\n",
      "\n",
      "======== chr21.rep2_cell_cycle ========\n",
      "chrom_ids            ndarray    (4558,)\n",
      "dna_zxys             ndarray    (4558, 651, 3)\n",
      "tss_zxys             ndarray    (4558, 84, 3)\n",
      "trans_pfs            ndarray    (4558, 84)\n",
      "G1_flags             ndarray    (4558,)\n",
      "G2/S_flags           ndarray    (4558,)\n",
      "\n",
      "======== BOTH REPS ========\n",
      "params               dict       0\n",
      "region_names         ndarray    (651,)\n",
      "mid_position_Mb      ndarray    (651,)\n",
      "start_position_Mb    ndarray    (651,)\n",
      "end_position_Mb      ndarray    (651,)\n",
      "gene_names           list       84\n",
      "\n",
      "\n",
      "Is 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\n",
      "chr21                     ✓\n",
      "chr21.rep1                ✓\n",
      "chr21.rep2_cell_cycle     ✓\n",
      "\n",
      "\n",
      "Confirm there aren't any weird rounding errors in 'start_position_Mb':\n",
      "chr21                     ✓\n",
      "chr21.rep1                ✓\n",
      "chr21.rep2_cell_cycle     ✓\n"
     ]
    }
   ],
   "source": [
    "# [GC+] Assess data structure\n",
    "in_common = False\n",
    "for name, data in all_data.items():\n",
    "    print(f'\\n======== {name} ========')\n",
    "    for key, val in data.items():\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            in_common = True\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "\n",
    "if in_common:\n",
    "    print(f'\\n======== BOTH REPS ========')\n",
    "    for key, val in data_rep1.items():\n",
    "        if not (key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key])):\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "            \n",
    "\n",
    "print(\"\\n\\nIs 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\")\n",
    "for name, data in all_data.items():\n",
    "    if 'chrom_ids' in data:\n",
    "        tmp = np.array_equal(data_rep1['chrom_ids'], np.arange(1, len(data_rep1['chrom_ids']) + 1))\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[tmp])\n",
    "\n",
    "print(\"\\n\\nConfirm there aren't any weird rounding errors in 'start_position_Mb':\")\n",
    "for name, data in all_data.items():\n",
    "    if 'start_position_Mb' in data:\n",
    "        tmp = (data['start_position_Mb'] - np.round(data['start_position_Mb'], 4)) != 0\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[~np.any(tmp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10400001 10500001 10600001 13250001 14000001 14050001 14100001 14150001\n",
      " 14200001 14250001]\n",
      "[10400000 10500000 10600000 13250000 14000000 14050000 14100000 14150000\n",
      " 14200000 14250000]\n",
      "[10.4  10.5  10.6  13.25 14.   14.05 14.1  14.15 14.2  14.25]\n",
      "[0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05 0.05]\n"
     ]
    }
   ],
   "source": [
    "print((region_starts)[:10])\n",
    "print((region_starts - 1)[:10])\n",
    "print(((region_starts - 1)[:10].astype(int) / 10 ** 6))\n",
    "print(np.round((region_starts - 1)[:10].astype(int) / 10 ** 6, 4) % 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.chrom_ids.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep1.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.gene_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.tss_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.trans_pfs.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.G1_flags.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr21/chr21.rep2_cell_cycle.G2-S_flags.npy\n"
     ]
    }
   ],
   "source": [
    "# [GC+] SAVE in-progress files - numpy\n",
    "\n",
    "for name, data in all_data.items():\n",
    "    for key, val in data.items():\n",
    "        if len(val) == 0 or key in ('start_position_Mb', 'end_position_Mb') or np.array_equal(np.asarray(val).ravel(), np.arange(np.asarray(val).size) + 1):\n",
    "            continue\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            tmp = chrom\n",
    "        else:\n",
    "            tmp = name\n",
    "        if isinstance(val, dict):\n",
    "            pass\n",
    "        else:\n",
    "            key = key.replace('/', '-')\n",
    "            print(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"))\n",
    "            np.save(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GC+] SAVE in-progress files - pickled\n",
    "with open(os.path.join(dir_3d_coords, rep1) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, rep2) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, chrom) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_combined, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "\n",
    "# with open(rep1_coords_file, 'rb') as handle:\n",
    "#     data_rep1 = pickle.load(handle)\n",
    "# with open(rep2_coords_file, 'rb') as handle:\n",
    "#     data_rep2 = pickle.load(handle)\n",
    "\n",
    "# with open(combo_coords_file, 'rb') as handle:\n",
    "#     data_combined = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1 generate imaging-based median distance and proximity frequency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# zxys_combined_list = np.array(data_combined['dna_zxys'])\n",
    "# distmap_combined_list = np.array([squareform(pdist(_zxy)) for _zxy in tqdm(zxys_combined_list)])\n",
    "\n",
    "# # generate median distance map\n",
    "# median_dist_matrix_combined = np.nanmedian(distmap_combined_list, axis = 0)\n",
    "# # generate contact map\n",
    "# contact_th = 500\n",
    "# contact_map_combined = np.sum(distmap_combined_list<contact_th, axis=0) / np.sum(np.isnan(distmap_combined_list)==False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f8419b994c4ab6b9415b50ace95fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd9572a92514ec09510e1ed5ca8fd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [GC+] edited so I can SAVE in-progress files\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "zxys_combined_list = np.array(data_combined['dna_zxys'])\n",
    "\n",
    "distmap_combined_list = np.concatenate([pdist(_zxy).reshape(1, -1) for _zxy in tqdm(zxys_combined_list)])\n",
    "np.save(distmap_list_file, distmap_combined_list) # [GC+] SAVE in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sc distances...\n",
      "chr21\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr21'\n",
    "sc_dist_vec = None\n",
    "# print('Loading sc distances...'); sc_dist_vec = np.load(files_dict[chrom]['distances.vector_per_cell'])\n",
    "print(chrom)\n",
    "process_sc_distances(files_dict[chrom], sc_dist_vec=sc_dist_vec, dis_mean=True, dis_median=True, contact_th=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "# distmap_combined_list = np.load(distmap_list_file)\n",
    "# median_dist_matrix_combined = np.load(median_dist_matrix_file)\n",
    "# contact_map_combined = np.load(median_dist_matrix_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1'></a>\n",
    "# Chromosome 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.gc'></a>\n",
    "## 1.gc Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2\n",
      "chr2.rep1\n",
      "chr2.rep2_p_arm\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr2'\n",
    "rep1_filename = files_3dcoords[chrom]['rep1']\n",
    "rep2_filename = files_3dcoords[chrom]['rep2']\n",
    "rep1 = files_dict[chrom]['rep1']\n",
    "rep2 = files_dict[chrom]['rep2']\n",
    "print(chrom); print(rep1); print(rep2)\n",
    "\n",
    "dir_3d_coords =  files_dict[chrom]['dir_coords3d']\n",
    "distmap_list_file = files_dict[chrom]['distances.vector_per_cell']\n",
    "median_dist_matrix_file = files_dict[chrom]['distances.median']\n",
    "mean_dist_matrix_file = files_dict[chrom]['distances.mean']\n",
    "mean_counts_matrix_c500_file = files_dict[chrom]['counts.mean.cutoff500']\n",
    "sc_counts_vec_c500_file = files_dict[chrom]['counts.vector_per_cell.cutoff500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1 load chr2 replicate 1 \n",
    "\n",
    "(entire chr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71554eb51e0d482e87db8e3b2d1f70df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep1_info_dict = {}\n",
    "with open(rep1_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep1_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep1_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep1 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep1_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep1_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep1_info_dict['Z(nm)'],rep1_info_dict['X(nm)'],\\\n",
    "                                         rep1_info_dict['Y(nm)'],rep1_info_dict['Genomic coordinate'],\\\n",
    "                                         rep1_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep1['chrom_ids'] = chr_ids\n",
    "data_rep1['region_names'] = region_names\n",
    "data_rep1['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep1['start_position_Mb'] = start_position_Mb\n",
    "data_rep1['end_position_Mb'] = end_position_Mb\n",
    "data_rep1['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep1_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep1_info_dict['Gene names'],\n",
    "                                                   rep1_info_dict['Transcription'],\n",
    "                                                   rep1_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep1_info_dict['Chromosome copy number'],\n",
    "                                                                  rep1_info_dict['Gene names'],\n",
    "                                                                  rep1_info_dict['Transcription'],\n",
    "                                                                  rep1_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep1['gene_names'] = gene_names\n",
    "    data_rep1['tss_zxys'] = tss_zxys_list\n",
    "    data_rep1['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep1_info_dict:\n",
    "    cell_cycle_types = np.unique(rep1_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep1_info_dict['Chromosome copy number'],rep1_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep1[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 1.2 load chr2 replicate 2 \n",
    "\n",
    "p-arm only, first 357 regions (loci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z(nm)', 'X(nm)', 'Y(nm)', 'Genomic coordinate', 'Chromosome copy number']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab24d76d64304a42b56008b43ff92529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load from file and extract info\n",
    "import csv\n",
    "rep2_info_dict = {}\n",
    "with open(rep2_filename, 'r') as _handle:\n",
    "    _reader = csv.reader(_handle, delimiter='\\t', quotechar='|')\n",
    "    _headers = next(_reader)\n",
    "    print(_headers)\n",
    "    # create keys for each header\n",
    "    for _h in _headers:\n",
    "        rep2_info_dict[_h] = []\n",
    "    # loop through content\n",
    "    for _contents in _reader:\n",
    "        for _h, _info in zip(_headers,_contents):\n",
    "            rep2_info_dict[_h].append(_info)\n",
    "\n",
    "# clean up information\n",
    "data_rep2 = {'params':{}}\n",
    "\n",
    "# clean up genomic coordiantes\n",
    "region_names = np.array([_n for _n in sorted(np.unique(rep2_info_dict['Genomic coordinate']), \n",
    "                                             key=lambda s:int(s.split(':')[1].split('-')[0]))])\n",
    "region_starts = np.array([int(_n.split(':')[1].split('-')[0]) for _n in region_names])\n",
    "region_ends = np.array([int(_n.split(':')[1].split('-')[1]) for _n in region_names])[np.argsort(region_starts)]\n",
    "region_starts = np.sort(region_starts)\n",
    "\n",
    "mid_positions = ((region_starts + region_ends)/2).astype(int)\n",
    "mid_positions_Mb = np.round(mid_positions / 1e6, 5) \n",
    "start_position_Mb = np.round(region_starts / 1e6, 5) \n",
    "end_position_Mb = np.round(region_ends / 1e6, 5) \n",
    "\n",
    "# clean up chrom copy number\n",
    "chr_nums = np.array([int(_info) for _info in rep2_info_dict['Chromosome copy number']])\n",
    "chr_ids, region_cts = np.unique(chr_nums, return_counts=True)\n",
    "dna_zxys_list = [[[] for _start in region_starts] for _id in chr_ids]\n",
    "\n",
    "# clean up zxy\n",
    "for _z,_x,_y,_reg_info, _cid in tqdm(zip(rep2_info_dict['Z(nm)'],rep2_info_dict['X(nm)'],\\\n",
    "                                         rep2_info_dict['Y(nm)'],rep2_info_dict['Genomic coordinate'],\\\n",
    "                                         rep2_info_dict['Chromosome copy number'])):\n",
    "    # get chromosome inds\n",
    "    _cid = int(_cid)\n",
    "    _cind = np.where(chr_ids == _cid)[0][0]\n",
    "    \n",
    "    # get region indices\n",
    "    _start = int(_reg_info.split(':')[1].split('-')[0])\n",
    "    _rind = np.where(region_starts==_start)[0][0]\n",
    "    \n",
    "    dna_zxys_list[_cind][_rind] = np.array([float(_z),float(_x), float(_y)])\n",
    "\n",
    "# merge together\n",
    "dna_zxys_list = np.array(dna_zxys_list)\n",
    "data_rep2['chrom_ids'] = chr_ids\n",
    "data_rep2['region_names'] = region_names\n",
    "data_rep2['mid_position_Mb'] = mid_positions_Mb\n",
    "data_rep2['start_position_Mb'] = start_position_Mb\n",
    "data_rep2['end_position_Mb'] = end_position_Mb\n",
    "data_rep2['dna_zxys'] = dna_zxys_list\n",
    "\n",
    "# clean up tss and transcription\n",
    "if 'Gene names' in rep2_info_dict:\n",
    "    import re\n",
    "    # first extract number of genes\n",
    "    gene_names = []\n",
    "    for _gene_info, _trans_info, _tss_coord in zip(rep2_info_dict['Gene names'],\n",
    "                                                   rep2_info_dict['Transcription'],\n",
    "                                                   rep2_info_dict['TSS ZXY(nm)']):\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            for _gene in _genes:\n",
    "                if _gene not in gene_names:\n",
    "                    gene_names.append(_gene)\n",
    "    print(f\"{len(gene_names)} genes exist in this dataset.\")\n",
    "    \n",
    "    # initialize gene and transcription\n",
    "    tss_zxys_list = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    transcription_profiles = [[[] for _gene in gene_names] for _id in chr_ids]\n",
    "    # loop through to get info\n",
    "    for _cid, _gene_info, _trans_info, _tss_locations in tqdm(zip(rep2_info_dict['Chromosome copy number'],\n",
    "                                                                  rep2_info_dict['Gene names'],\n",
    "                                                                  rep2_info_dict['Transcription'],\n",
    "                                                                  rep2_info_dict['TSS ZXY(nm)'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        # process if there are genes in this region:\n",
    "        if _gene_info != '':\n",
    "            # split by semicolon\n",
    "            _genes = _gene_info.split(';')[:-1]\n",
    "            _transcribes = _trans_info.split(';')[:-1]\n",
    "            _tss_zxys = _tss_locations.split(';')[:-1]\n",
    "            for _gene, _transcribe, _tss_zxy in zip(_genes, _transcribes, _tss_zxys):\n",
    "                # get gene index\n",
    "                _gind = gene_names.index(_gene)\n",
    "                # get transcription profile\n",
    "                if _transcribe == 'on':\n",
    "                    transcription_profiles[_cind][_gind] = True\n",
    "                else:\n",
    "                    transcription_profiles[_cind][_gind] = False\n",
    "                # get coordinates\n",
    "                _tss_zxy = np.array([np.float(_c) for _c in re.split(r'\\s+', _tss_zxy.split('[')[1].split(']')[0]) if _c != ''])\n",
    "                tss_zxys_list[_cind][_gind] = _tss_zxy\n",
    "                \n",
    "    tss_zxys_list = np.array(tss_zxys_list)\n",
    "    transcription_profiles = np.array(transcription_profiles)\n",
    "    data_rep2['gene_names'] = gene_names\n",
    "    data_rep2['tss_zxys'] = tss_zxys_list\n",
    "    data_rep2['trans_pfs'] = transcription_profiles\n",
    "\n",
    "# clean up cell_cycle states\n",
    "if 'Cell cycle state' in rep2_info_dict:\n",
    "    cell_cycle_types = np.unique(rep2_info_dict['Cell cycle state'])\n",
    "    cell_cycle_flag_dict = {_k:[[] for _id in chr_ids] for _k in cell_cycle_types if _k != 'ND'}\n",
    "    for _cid, _state in tqdm(zip(rep2_info_dict['Chromosome copy number'],rep2_info_dict['Cell cycle state'])):\n",
    "        # get chromosome inds\n",
    "        _cid = int(_cid)\n",
    "        _cind = np.where(chr_ids == _cid)[0][0]\n",
    "        if np.array([_v[_cind]==[] for _k,_v in cell_cycle_flag_dict.items()]).any():\n",
    "            for _k,_v in cell_cycle_flag_dict.items():\n",
    "                if _k == _state:\n",
    "                    _v[_cind] = True\n",
    "                else:\n",
    "                    _v[_cind] = False\n",
    "    # append to data\n",
    "    for _k, _v in cell_cycle_flag_dict.items():\n",
    "        data_rep2[f'{_k}_flags'] = np.array(_v)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Save current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {rep1: data_rep1, rep2: data_rep2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== chr2.rep1 ========\n",
      "chrom_ids            ndarray    (3029,)\n",
      "region_names         ndarray    (935,)\n",
      "mid_position_Mb      ndarray    (935,)\n",
      "start_position_Mb    ndarray    (935,)\n",
      "end_position_Mb      ndarray    (935,)\n",
      "dna_zxys             ndarray    (3029, 935, 3)\n",
      "\n",
      "======== chr2.rep2_p_arm ========\n",
      "chrom_ids            ndarray    (4848,)\n",
      "region_names         ndarray    (357,)\n",
      "mid_position_Mb      ndarray    (357,)\n",
      "start_position_Mb    ndarray    (357,)\n",
      "end_position_Mb      ndarray    (357,)\n",
      "dna_zxys             ndarray    (4848, 357, 3)\n",
      "\n",
      "======== BOTH REPS ========\n",
      "params               dict       0\n",
      "\n",
      "\n",
      "Is 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\n",
      "chr2.rep1           : True\n",
      "chr2.rep2_p_arm     : True\n",
      "\n",
      "\n",
      "Are there weird rounding errors in 'start_position_Mb'?\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# [GC+] Assess data structure\n",
    "in_common = False\n",
    "for name, data in all_data.items():\n",
    "    print(f'\\n======== {name} ========')\n",
    "    for key, val in data.items():\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            in_common = True\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "\n",
    "if in_common:\n",
    "    print(f'\\n======== BOTH REPS ========')\n",
    "    for key, val in data_rep1.items():\n",
    "        if not (key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key])):\n",
    "            continue\n",
    "        if isinstance(val, np.ndarray):\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {val.shape}')\n",
    "        else:\n",
    "            print(f'{key.ljust(20, \" \")} {type(val).__name__.ljust(10, \" \")} {len(val)}')\n",
    "            \n",
    "\n",
    "print(\"\\n\\nIs 'chrom_ids' just an index, eg. np.arange(1, len(chrom_ids) + 1)'?\")\n",
    "for name, data in all_data.items():\n",
    "    if 'chrom_ids' in data:\n",
    "        tmp = np.array_equal(data_rep1['chrom_ids'], np.arange(1, len(data_rep1['chrom_ids']) + 1))\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[tmp])\n",
    "\n",
    "print(\"\\n\\nConfirm there aren't any weird rounding errors in 'start_position_Mb':\")\n",
    "for name, data in all_data.items():\n",
    "    if 'start_position_Mb' in data:\n",
    "        tmp = (data['start_position_Mb'] - np.round(data['start_position_Mb'], 4)) != 0\n",
    "        print(f\"{name.ljust(25, ' ')} \" + {True: '✓', False: '✗'}[~np.any(tmp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep1.dna_zxys.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.region_names.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.mid_position_Mb.npy\n",
      "/net/noble/vol5/user/gesine/repos/Chromatin_Analysis_2020_cell/sequential_tracing/data/in_progress/3d_coords/chr2/chr2.rep2_p_arm.dna_zxys.npy\n"
     ]
    }
   ],
   "source": [
    "# [GC+] SAVE in-progress files - numpy\n",
    "\n",
    "for name, data in all_data.items():\n",
    "    for key, val in data.items():\n",
    "        if len(val) == 0 or key in ('start_position_Mb', 'end_position_Mb') or np.array_equal(np.asarray(val).ravel(), np.arange(np.asarray(val).size) + 1):\n",
    "            continue\n",
    "        if key in data_rep1 and key in data_rep2 and np.array_equal(data_rep1[key], data_rep2[key]):\n",
    "            tmp = chrom\n",
    "        else:\n",
    "            tmp = name\n",
    "        if isinstance(val, dict):\n",
    "            pass\n",
    "        else:\n",
    "            key = key.replace('/', '-')\n",
    "            print(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"))\n",
    "            np.save(os.path.join(dir_3d_coords, f\"{tmp}.{key}.npy\"), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GC+] SAVE in-progress files - pickled\n",
    "with open(os.path.join(dir_3d_coords, rep1) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(dir_3d_coords, rep2) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_rep2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "\n",
    "# with open(rep1_coords_file, 'rb') as handle:\n",
    "#     data_rep1 = pickle.load(handle)\n",
    "# with open(rep2_coords_file, 'rb') as handle:\n",
    "#     data_rep2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.0 corresponding regions (loci) for p and q arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # P and q arm crop\n",
    "# p_crop = slice(0, 357)\n",
    "# q_crop = slice(357, len(data_rep1['dna_zxys'][0]))\n",
    "# print(p_crop, q_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='2.1'></a>\n",
    "## 2.1 generate imaging-based median distance and proximity frequency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import squareform, pdist\n",
    "# zxys_rep1_list = np.array(data_rep1['dna_zxys'])\n",
    "# distmap_rep1_list = np.array([squareform(pdist(_zxy)) for _zxy in zxys_rep1_list])\n",
    "# # calculate contact freq map\n",
    "# contact_th = 500\n",
    "# contact_rep1_map = np.sum(distmap_rep1_list<contact_th, axis=0) / np.sum(np.isnan(distmap_rep1_list)==False, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69317f56bf404b67816c987021d07654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968a5249b4849e5acdf5f7e19ae9cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [GC+] edited so I can SAVE in-progress files\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "zxys_rep1_list = np.array(data_rep1['dna_zxys'])\n",
    "\n",
    "distmap_rep1_list = np.concatenate([pdist(_zxy).reshape(1, -1) for _zxy in tqdm(zxys_rep1_list)])\n",
    "np.save(distmap_list_file, distmap_rep1_list) # [GC+] SAVE in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr2\n",
      "Loading sc distances...\n",
      "Generate median distances across cells...\n",
      "Generate mean distances across cells...\n",
      "Generate contacts...\n",
      "                 ...saving contacts<thresh per cell\n",
      "                 ...saving mean contacts across cells\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "chrom = 'chr2'\n",
    "print(chrom)\n",
    "sc_dist_vec = None\n",
    "# print('Loading sc distances...'); sc_dist_vec = np.load(files_dict[chrom]['distances.vector_per_cell'])\n",
    "process_sc_distances(files_dict[chrom], sc_dist_vec=sc_dist_vec, dis_mean=True, dis_median=True, contact_th=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.gc Load in-progress files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [GC+] LOAD in-progress files\n",
    "# distmap_combined_list = np.load(distmap_list_file)\n",
    "# median_dist_matrix_combined = np.load(median_dist_matrix_file)\n",
    "# contact_map_combined = np.load(median_dist_matrix_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create proper hiclib-style counts matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict[chrom].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check, make sure all regions (loci) are 0.05 Mb\n",
    "replicate = 'proceed_with'\n",
    "\n",
    "for chrom in files_dict.keys():\n",
    "    region_names = np.load(os.path.join(files_dict[chrom]['dir_coords3d'], files_dict[chrom][replicate] + '.region_names.npy'))\n",
    "    regions = np.concatenate([np.array(re.split(r'[:-]', x)[1:], dtype=int).reshape(1, -1) for x in region_names])\n",
    "    region_sizes = np.unique(regions[:,1] - regions[:,0]) / 1e6\n",
    "    if region_sizes.size != 1 or region_sizes[0] != 0.05:\n",
    "        raise ValueError('Region sizes not all equal to 0.05 Mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check, make sure there aren't any beads that are NaN on one axis but not all axes\n",
    "replicate = 'proceed_with'\n",
    "\n",
    "for chrom in files_dict.keys():\n",
    "    dna_zxys = np.load(os.path.join(files_dict[chrom]['dir_coords3d'], files_dict[chrom][replicate] + '.dna_zxys.npy'))\n",
    "    tmp = np.isnan(dna_zxys).sum(2) \n",
    "    irregular_nans = np.sum((tmp != 0) & (tmp != 3))\n",
    "    if irregular_nans != 0:\n",
    "        raise ValueError('Some beads have NaNs one one axis but not all axes!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "# from scipy.spatial.distance import squareform\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore', message='', category=UserWarning)\n",
    "    warnings.filterwarnings('ignore', message='', category=FutureWarning)\n",
    "    from iced.io import write_counts, write_lengths\n",
    "\n",
    "def get_complete_counts_matrix(files_dict, chrom, locus_cutoff=0.99, cell_cutoff=0.99, res=0.05,\n",
    "                               load_counts=True, save_counts=False, sc_counts_dict=None, \n",
    "                               replicate='proceed_with', verbose=False):\n",
    "    if verbose: print(f\"==== chrom={chrom}, cell_th={cell_cutoff}, locus_th={locus_cutoff}\")\n",
    "    \n",
    "    # Load data... shape=(ncells, nloci, 3)\n",
    "    dna_zxys = np.load(os.path.join(files_dict[chrom]['dir_coords3d'], files_dict[chrom][replicate] + '.dna_zxys.npy'))\n",
    "    nloci_orig = dna_zxys.shape[1]\n",
    "    \n",
    "    # Only use cells with >= cell_cutoff*100% non-NaN loci\n",
    "    cells_good = np.mean(~np.isnan(dna_zxys[:, :, 0]), axis=1) >= cell_cutoff # Ratio of good-bins-per-cell >= cell_cutoff \n",
    "    perc_cells_retained = np.mean(cells_good) * 100\n",
    "    dna_zxys = dna_zxys[cells_good, :, :] # Filter out 'bad cells'\n",
    "    \n",
    "    # Only use loci with >= locus_cutoff*100% non-NaN cells\n",
    "    loci_good = np.mean(~np.isnan(dna_zxys[:, :, 0]), axis=0) >= locus_cutoff\n",
    "    perc_mappable_loci_retained = np.mean(loci_good) * 100 # This is the % of MAPPABLE loci discarded\n",
    "    dna_zxys = dna_zxys[:, loci_good, :] # Filter out 'bad loci'\n",
    "\n",
    "    # Get mids that pass locus_cutoff\n",
    "    mids_orig = np.load(os.path.join(files_dict[chrom]['dir_coords3d'], files_dict[chrom][replicate] + '.mid_position_Mb.npy'))\n",
    "    if not np.array_equal(mids_orig, np.sort(mids_orig)):\n",
    "        raise ValueError('Midpoints not in ascending order')\n",
    "    mids_filter = mids_orig[loci_good]\n",
    "    \n",
    "    # Compute % loci retained: This is the % of ALL loci retained, excluding any unappable loci at telomeres\n",
    "    mids_all_untrunc = np.round(np.arange(mids_orig.min(), mids_orig.max() + res, res), 6)\n",
    "    perc_untrunc_loci_retained = mids_filter.size / mids_all_untrunc.size * 100\n",
    "    \n",
    "    # Compute % loci retained: This is the % of ALL loci retained, excluding any unappable loci at telomeres...\n",
    "    # ... AND excluding any filtered-out loci at telomeres\n",
    "    mids_all = np.round(np.arange(mids_filter.min(), mids_filter.max() + res, res), 6)\n",
    "    perc_loci_retained = mids_filter.size / mids_all.size * 100\n",
    "\n",
    "    res = {'chrom': chrom, 'cell_cutoff': cell_cutoff, 'locus_cutoff': locus_cutoff,\n",
    "           'perc_cells_retained': perc_cells_retained, 'perc_mappable_loci_retained': perc_mappable_loci_retained,\n",
    "           'perc_loci_retained': perc_loci_retained, 'perc_untrunc_loci_retained': perc_untrunc_loci_retained,\n",
    "           'ncells': cells_good.sum(), 'nloci_mappable': loci_good.sum(), 'nloci': mids_orig.size,\n",
    "           }\n",
    "        \n",
    "    if load_counts:\n",
    "        # Load counts per cell\n",
    "        if sc_counts_dict is None:\n",
    "            if verbose: print(f\"\\tLoading counts per cell for {chrom}...\")\n",
    "            sc_counts_dict = np.load(os.path.join(files_dict[chrom]['counts.vector_per_cell.cutoff500']))\n",
    "        \n",
    "        # Get counts that pass cell cutoff, & sum across cells\n",
    "        if verbose: print(\"\\tSum counts across cells that pass cutoff...\")\n",
    "        counts_orig = np.zeros([nloci_orig, nloci_orig], dtype=int)\n",
    "        counts_orig[np.triu_indices(nloci_orig, 1)] = np.sum(sc_counts_dict[cells_good, :])\n",
    "        \n",
    "        # Get counts that pass locus cutoff\n",
    "        if verbose: print(\"\\tFilter counts bins to loci that pass cutoff...\")\n",
    "        counts_filter = counts_orig[loci_good][:,loci_good]\n",
    "        \n",
    "        res['reads_total'] = counts_filter.sum()\n",
    "        res['reads_perbin'] = counts_filter[np.triu_indices(counts_filter.shape[0], 1)].mean()\n",
    "\n",
    "        # Create complete counts matrix\n",
    "        counts = np.zeros((mids_all.size, mids_all.size), dtype=int)\n",
    "        idx = np.where(np.isin(mids_all, mids_filter))[0]\n",
    "        counts[idx][:,idx] = counts_filter\n",
    "\n",
    "        if save_counts:\n",
    "            outdir = os.path.join(counts_folder, f\"{chrom}.cell{cell_cutoff * 100:g}p.locus{locus_cutoff * 100:g}p\")\n",
    "            write_counts(os.path.join(outdir, \"counts.matrix\"), counts)\n",
    "            write_lengths(os.path.join(outdir, \"counts.bed\"), np.array([counts.shape[0]]))\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def compare_cell_locus_cutoffs(files_dict, list_cell_cutoff, list_locus_cutoff, list_chrom=None,\n",
    "                               sc_counts_dict=None, res_all=None, verbose=True):\n",
    "    if list_chrom is None:\n",
    "        list_chrom = files_dict.keys()\n",
    "    elif isinstance(list_chrom, str):\n",
    "        list_chrom = [list_chrom]\n",
    "    \n",
    "    if sc_counts_dict is None:\n",
    "        sc_counts_dict = {}\n",
    "    for chrom in list_chrom:\n",
    "        if chrom not in sc_counts_dict:\n",
    "            if verbose: print(f\"Loading counts per cell for {chrom}...\")\n",
    "            sc_counts_dict[chrom] = np.load(os.path.join(files_dict[chrom]['counts.vector_per_cell.cutoff500']))\n",
    "\n",
    "    if res_all is None:\n",
    "        res_all = {}\n",
    "    for chrom in list_chrom:\n",
    "        # Without cutoffs\n",
    "        key = f\"{chrom} 0 0\"\n",
    "        if key not in res_all:\n",
    "            res_all[key] = get_complete_counts_matrix(\n",
    "                files_dict, chrom=chrom, cell_cutoff=0, locus_cutoff=0,\n",
    "                sc_counts_dict=sc_counts_dict[chrom], verbose=verbose)\n",
    "        # With cutoffs\n",
    "        for locus_cutoff in list_locus_cutoff:\n",
    "            for cell_cutoff in list_cell_cutoff:\n",
    "                key = f\"{chrom} {cell_cutoff} {locus_cutoff}\"\n",
    "                if key not in res_all:\n",
    "                    res_all[key] = get_complete_counts_matrix(\n",
    "                        files_dict, chrom=chrom, cell_cutoff=cell_cutoff, locus_cutoff=locus_cutoff,\n",
    "                        sc_counts_dict=sc_counts_dict[chrom], verbose=verbose)\n",
    "\n",
    "    return res_all, sc_counts_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_counts_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== chrom=chr21, cell_th=0.5, locus_th=0.99\n",
      "\tSum counts across cells that pass cutoff...\n",
      "\tFilter counts bins to loci that pass cutoff...\n"
     ]
    }
   ],
   "source": [
    "cutoff_combos = [\n",
    "    {'cell': [0.99, 0.999], 'locus': [0.99, 0.999]},\n",
    "    {'cell': [0.9, 0.95], 'locus': [0.99]},\n",
    "    {'cell': [0.5], 'locus': [0.99]},\n",
    "]\n",
    "\n",
    "for combo in cutoff_combos:\n",
    "    res_all, sc_counts_dict = compare_cell_locus_cutoffs(\n",
    "        files_dict=files_dict, list_cell_cutoff=combo['cell'],\n",
    "        list_locus_cutoff=combo['locus'], list_chrom='chr21',\n",
    "        sc_counts_dict=sc_counts_dict, res_all=res_all, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>CELL_th</th>\n",
       "      <th>LOCUS_th</th>\n",
       "      <th>locus_cov</th>\n",
       "      <th>locus_cov_untrunc</th>\n",
       "      <th>ncells</th>\n",
       "      <th>nbins</th>\n",
       "      <th>%cells removed</th>\n",
       "      <th>%regions removed</th>\n",
       "      <th>nreads_total</th>\n",
       "      <th>nreads_perbin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>12149</td>\n",
       "      <td>651</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>6.6e13</td>\n",
       "      <td>3.1e08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1.97%</td>\n",
       "      <td>0.551%</td>\n",
       "      <td>12080</td>\n",
       "      <td>4</td>\n",
       "      <td>0.568%</td>\n",
       "      <td>99.4%</td>\n",
       "      <td>1.9e09</td>\n",
       "      <td>3.1e08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.990</td>\n",
       "      <td>11.3%</td>\n",
       "      <td>9.09%</td>\n",
       "      <td>10929</td>\n",
       "      <td>66</td>\n",
       "      <td>10%</td>\n",
       "      <td>89.9%</td>\n",
       "      <td>6.2e11</td>\n",
       "      <td>2.9e08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.990</td>\n",
       "      <td>28.8%</td>\n",
       "      <td>24.5%</td>\n",
       "      <td>7967</td>\n",
       "      <td>178</td>\n",
       "      <td>34.4%</td>\n",
       "      <td>72.7%</td>\n",
       "      <td>3.4e12</td>\n",
       "      <td>2.2e08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.990</td>\n",
       "      <td>88.6%</td>\n",
       "      <td>81.7%</td>\n",
       "      <td>2078</td>\n",
       "      <td>593</td>\n",
       "      <td>82.9%</td>\n",
       "      <td>8.91%</td>\n",
       "      <td>1.1e13</td>\n",
       "      <td>6.2e07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.999</td>\n",
       "      <td>21.1%</td>\n",
       "      <td>17.8%</td>\n",
       "      <td>2078</td>\n",
       "      <td>129</td>\n",
       "      <td>82.9%</td>\n",
       "      <td>80.2%</td>\n",
       "      <td>5.1e11</td>\n",
       "      <td>6.2e07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.990</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>168</td>\n",
       "      <td>651</td>\n",
       "      <td>98.6%</td>\n",
       "      <td>0%</td>\n",
       "      <td>1.2e12</td>\n",
       "      <td>5.6e06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr21</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>89.7%</td>\n",
       "      <td>168</td>\n",
       "      <td>651</td>\n",
       "      <td>98.6%</td>\n",
       "      <td>0%</td>\n",
       "      <td>1.2e12</td>\n",
       "      <td>5.6e06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chrom  CELL_th  LOCUS_th locus_cov locus_cov_untrunc  ncells  nbins  \\\n",
       "0  chr21    0.000     0.000     89.7%             89.7%   12149    651   \n",
       "7  chr21    0.500     0.990     1.97%            0.551%   12080      4   \n",
       "5  chr21    0.900     0.990     11.3%             9.09%   10929     66   \n",
       "6  chr21    0.950     0.990     28.8%             24.5%    7967    178   \n",
       "1  chr21    0.990     0.990     88.6%             81.7%    2078    593   \n",
       "3  chr21    0.990     0.999     21.1%             17.8%    2078    129   \n",
       "2  chr21    0.999     0.990     89.7%             89.7%     168    651   \n",
       "4  chr21    0.999     0.999     89.7%             89.7%     168    651   \n",
       "\n",
       "  %cells removed %regions removed nreads_total nreads_perbin  \n",
       "0             0%               0%       6.6e13        3.1e08  \n",
       "7         0.568%            99.4%       1.9e09        3.1e08  \n",
       "5            10%            89.9%       6.2e11        2.9e08  \n",
       "6          34.4%            72.7%       3.4e12        2.2e08  \n",
       "1          82.9%            8.91%       1.1e13        6.2e07  \n",
       "3          82.9%            80.2%       5.1e11        6.2e07  \n",
       "2          98.6%               0%       1.2e12        5.6e06  \n",
       "4          98.6%               0%       1.2e12        5.6e06  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(res_all.values())\n",
    "df2 = df.copy()\n",
    "for col in [c for c in df2.columns if 'perc_' in c]:\n",
    "    df2[col] = df2[col].map('{:,.3g}%'.format)\n",
    "for col in [c for c in df2.columns if 'reads_' in c]:\n",
    "    df2[col] = df2[col].map('{:,.2g}'.format).str.replace('e\\+', 'e', regex=True)\n",
    "df2.sort_values(['chrom', 'cell_cutoff', 'locus_cutoff'], inplace=True)\n",
    "if len(df2.chrom.unique()) == 1:\n",
    "    dr2.drop('chrom', axis=1, inplace=True)\n",
    "renaming_tmp = {c: c.replace('perc_', '%').replace('_retained', '') for c in df2.columns if 'perc_' in c}\n",
    "df2.rename(renaming_tmp, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Are imaged loci contiguous across mappable regions of the chromosome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 0.05 # 0.05 Mb (50 kb)\n",
    "replicate = 'proceed_with'\n",
    "chrom = 'chr2'\n",
    "chrom = 'chr21'\n",
    "\n",
    "mids_orig = np.load(os.path.join(files_dict[chrom]['dir_coords3d'], files_dict[chrom][replicate] + '.mid_position_Mb.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2, 53, 15,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_next_bin = np.round((mids_orig[1:] - mids_orig[:-1]) / res, 6)\n",
    "if np.allclose(to_next_bin, to_next_bin.astype(int)):\n",
    "    to_next_bin = to_next_bin.astype(int)\n",
    "\n",
    "to_next_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_next_bin.min()=1\n",
      "[[  0   2]\n",
      " [  1   2]\n",
      " [  2  53]\n",
      " [  3  15]\n",
      " [ 66   5]\n",
      " [162   2]\n",
      " [275   2]\n",
      " [581   2]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"to_next_bin.min()={to_next_bin.min()}\")\n",
    "gaps_mask = np.where(to_next_bin != to_next_bin.min())[0]\n",
    "print(np.stack([gaps_mask, to_next_bin[gaps_mask]], axis=1))\n",
    "# print(to_next_bin[gaps_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mids_all_untrunc = np.round(np.arange(mids_orig.min(), mids_orig.max() + res, res), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # P and q arm crop\n",
    "# p_crop = slice(0, 357)\n",
    "# q_crop = slice(357, len(data_rep1['dna_zxys'][0]))\n",
    "# print(p_crop, q_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "slice(0, 10, None)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice(0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
